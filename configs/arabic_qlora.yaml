# LoRA + 8-bit optimizer for whisper-large-v3-turbo on Arabic (memory-reduced).
# NOTE: True QLoRA (4-bit base model) is NOT implemented in this codebase.
# This config uses LoRA + 8-bit AdamW for lower memory; base model stays FP32.
# Replace with your Arabic HF datasets.

model:
  init_name: large-v3-turbo
  bfloat16: false
  lora: true
  lora_config:
    rank: 16
    lora_alpha: 32
    lora_dropout: 0.1

dataset:
  train_datasets:
    - your-org/arabic-asr-train
  select_n_per_t_ds: [null]
  groupby_col: [null]
  val_datasets:
    - your-org/arabic-asr-val
  val_dataset_names: [arabic_val]
  select_n_per_v_ds: [500]
  train_split_name: train
  valid_split_name: validation
  default_language: ar
  language: ar
  no_timestamp_training: false
  max_prompt_length: 223
  prompt_use_rate: 0.5
  no_timestamp_rate: 0.5
  batch_size: 32
  batch_size_eval: 32

lr_scheduler:
  type: linear
  warmup_steps: 128

optimizer:
  type: adamw
  8bit: true
  params:
    lr: 2.0e-4
    weight_decay: 0.1
    betas: [0.9, 0.98]
    eps: 1.0e-6
    amsgrad: false

training:
  accum_grad_steps: 4
  label_smoothing: 0.05
  train_only_decoder: false
  train_only_encoder: false
  max_grad_norm: 1.0
  stochastic_depth: 0.1
  epochs: 2
  eval_steps: 0.25
  save_all_checkpoints: false
  max_train_loss: 25
  mixed_precision_training: true
  mp_dtype: fp16
  gradient_checkpointing_encoder: true
  gradient_checkpointing_decoder: true
  measure_inference_latency: true

augmentation:
  spec_augment:
    apply: true
    time_mask_param: 100
    p: 1.0
    freq_mask_param: 43
    time_warp_w: 80
  deep_spec_augment:
    apply: true
    time_mask_param: 100
    freq_mask_param: 27
    layer_indices: null
  bpe_dropout: 0.1
  extremes_spec_augment:
    apply: false
  audio_augment:
    apply_office_aug: true
    apply_baseline_aug: true

seed: 123
save_dir: output
